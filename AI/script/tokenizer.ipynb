{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然言語処理の一連の流れ\n",
    "\n",
    "  1. Tokenizer（形態素解析）をし、文章をfastTextで解析出来る形へと変換する。\n",
    "  2. fastTextで文章データをベクトル化（複数の数字に変換すること）する。\n",
    "  3. ベクトル化した文章データを、学習器（今回はディープラーニングを使用する）で処理させる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目的\n",
    "\n",
    "Tokenizerしたデータを保存する\n",
    "以下を残す\n",
    "\n",
    "  * 再生数\n",
    "  * コメント数\n",
    "  * マイリスト数\n",
    "  * 動画タイトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本 False\n",
      "記事 False\n",
      "で False\n",
      "は False\n",
      "  True\n",
      "Google False\n",
      "  True\n",
      "の False\n",
      "  True\n",
      "BERT False\n",
      "  True\n",
      "について False\n",
      "、 True\n",
      "その False\n",
      "概要 False\n",
      "を False\n",
      "紹介 False\n",
      "し False\n",
      "、 True\n",
      "BERT False\n",
      "  True\n",
      "の False\n",
      "事前 False\n",
      "学習 False\n",
      "済み False\n",
      "モデル False\n",
      "を False\n",
      "用い False\n",
      "て False\n",
      "ファインチューニング False\n",
      "により False\n",
      "独自 False\n",
      "の False\n",
      "モデル False\n",
      "を False\n",
      "構築 False\n",
      "する False\n",
      "こと False\n",
      "を False\n",
      "念頭 False\n",
      "に False\n",
      "、 True\n",
      "BERT False\n",
      "  True\n",
      "の False\n",
      "入出力 False\n",
      "インタフェース False\n",
      "や False\n",
      "学習 False\n",
      "データ False\n",
      "の False\n",
      "構造 False\n",
      "を False\n",
      "説明 False\n",
      "し False\n",
      "ます False\n",
      "。 True\n",
      "そして False\n",
      "、 True\n",
      "ファインチューニング False\n",
      "により False\n",
      "独自 False\n",
      "の False\n",
      "モデル False\n",
      "を False\n",
      "構築 False\n",
      "する False\n",
      "例 False\n",
      "として False\n",
      "、 True\n",
      "chABSA False\n",
      "  True\n",
      "データセット False\n",
      "を False\n",
      "用い False\n",
      "た False\n",
      "感情 False\n",
      "分析 False\n",
      "モデル False\n",
      "生成 False\n",
      "の False\n",
      "実験 False\n",
      "結果 False\n",
      "および False\n",
      "アプリケーション False\n",
      "から False\n",
      "利用 False\n",
      "する False\n",
      "際 False\n",
      "の False\n",
      "ポイント False\n",
      "を False\n",
      "紹介 False\n",
      "し False\n",
      "ます False\n",
      "。 True\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "\n",
    "s = '本記事では Google の BERT について、その概要を紹介し、BERT の事前学習済みモデルを用いてファインチューニングにより独自のモデルを構築することを念頭に、BERT の入出力インタフェースや学習データの構造を説明します。そして、ファインチューニングにより独自のモデルを構築する例として、chABSA データセットを用いた感情分析モデル生成の実験結果およびアプリケーションから利用する際のポイントを紹介します。'\n",
    "\n",
    "for token in t.tokenize(s):\n",
    "    print(token.surface, token.part_of_speech.split(\",\")[0] == \"記号\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in t.tokenize('需要あるでしょうか。セガサターン SEGA', wakati=True):\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
